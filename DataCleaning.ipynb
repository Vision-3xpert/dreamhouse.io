{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import folium as fl\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderUnavailable, GeocoderQuotaExceeded\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import re\n",
    "quartier_url = 'https://data.montreal.ca/dataset/00bd85eb-23aa-4669-8f1b-ba9a000e3dd8/resource/e9b0f927-8f75-458c-8fda-b5da65cc8b73/download/limadmin.geojson'\n",
    "Montreal = []\n",
    "\n",
    "try:\n",
    "    r2 = requests.get(quartier_url, timeout=30)\n",
    "\n",
    "except requests.exceptions.RequestException as erreur2:\n",
    "    print(f\"Erreur de connexion à l'adresse web suivante {quartier_url} : {erreur2}\")\n",
    "\n",
    "else:\n",
    "    if r2.status_code >= 200 and r2.status_code <= 299:\n",
    "        Montreal = r2.json()\n",
    "    else:\n",
    "        print(f\"Erreur de connexion à l'adresse web suivante {quartier_url} : erreur {r2.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ameneties\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#italian restaurants\n",
    "italien = pd.read_csv('CSVs/Restaurant-Italien.csv')\n",
    "new_italien = pd.read_csv('CSVs/new-towns-italien.csv')\n",
    "#TODO  Rajouter le type de restaurant correspondant\n",
    "italien[[\"Type\"]] = \"italien\"\n",
    "new_italien[[\"Type\"]] = \"italien\"\n",
    "Italy = pd.concat([italien,new_italien])\n",
    "Italy = Italy.drop_duplicates()\n",
    "\n",
    "#greek restaurants\n",
    "Grec = pd.read_csv('CSVs/Restaurant-Grec.csv')\n",
    "new_greek = pd.read_csv('CSVs/new-towns-greek.csv')\n",
    "Grec[[\"Type\"]] =  \"grec\"\n",
    "new_greek[[\"Type\"]] = \"grec\"\n",
    "Greek = pd.concat([Grec,new_greek])\n",
    "Greek = Greek.drop_duplicates()\n",
    "\n",
    "\n",
    "#African Restaurants\n",
    "Africain = pd.read_csv('CSVs/Restaurant Africain.csv')\n",
    "Africain[[\"Type\"]] = \"africain\"\n",
    "\n",
    "\n",
    "#Antillais Restaurants\n",
    "Antillais = pd.read_csv('CSVs/Restaurant-Antillais.csv')\n",
    "Antillais[[\"Type\"]] =  \"antillais\"\n",
    "\n",
    "#Asian Restaurants\n",
    "Asiatique = pd.read_csv('CSVs/Restaurant-Asiatique.csv')\n",
    "new_asian = pd.read_csv(\"CSVs/new-towns-asian.csv\")\n",
    "Asiatique[[\"Type\"]] =  \"asiatique\"\n",
    "new_asian[[\"Type\"]] = \"asiatique\"\n",
    "Asian = pd.concat([Asiatique,new_asian])\n",
    "Asian = Asian.drop_duplicates()\n",
    "\n",
    "#Mexican Restaurants\n",
    "Mexicain = pd.read_csv('CSVs/Restaurant-Mexicain.csv')\n",
    "Mexicain[[\"Type\"]] = \"mexicain\"\n",
    "\n",
    "#Healthy Restaurants\n",
    "Santé = pd.read_csv('CSVs/Restaurant-Santé.csv')\n",
    "new_healthy = pd.read_csv(\"CSVs/new-towns-healthy.csv\")\n",
    "Santé[[\"Type\"]] = \"santé\"\n",
    "new_healthy[[\"Type\"]] = \"santé\"\n",
    "Healthy = pd.concat([Santé,new_healthy])\n",
    "Healthy = Healthy.drop_duplicates()\n",
    "\n",
    "#Coffee place\n",
    "Café = pd.read_csv('CSVs/Café.csv')\n",
    "new_coffee = pd.read_csv(\"CSVs/new-towns-coffee.csv\")\n",
    "Café[[\"Type\"]] = \"café\"\n",
    "new_coffee[[\"Type\"]] = \"café\"\n",
    "Coffee = pd.concat([Café,new_coffee])\n",
    "Coffee = Coffee.drop_duplicates()\n",
    "#All Restaurants\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Neighbourhoods Data ( GPS coordinates + Area and Population)\n",
    "neighborhoods = gpd.read_file(\"MTLTieks.geojson\")\n",
    "superficie = pd.read_csv('CSVs/Superficie_quartier.csv')\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Properties\n",
    "Propriétés = pd.read_excel('CSVs/Centris_scraping2.xlsx')\n",
    "## Clean the address list from Initial Property list\n",
    "Addresse_list = Propriétés['Addresse'].tolist()\n",
    "for i in range(len(Addresse_list)):\n",
    "   ##TODO Retirer toute l'information qui vient après \", Montréal\" (mettons (123 avenue xyz, Montréal, XYZ) --> (123, avenue xyz, Montréal))\n",
    "   Addresse_list[i] = (Addresse_list[i].split(', Montréal')[0]+\", Montréal\") \n",
    "\n",
    "   ## Retirer toutes les lettres qui suiveront des numéro civique (Exemple 123A de la gauchetière --> 123 de la gauchetière)\n",
    "   Addresse_list[i] = re.sub(r'\\d+[A-Za-z]*', lambda x: x.group().rstrip('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdfghijklmnopqrstuvwxyz'), Addresse_list[i])   # address = Addresse_list[i]\n",
    "\n",
    "   ## Ajouter un \"e\" lorsque nous avons des cas comme '20 rue --> 20e rue OU 8 avenue --> 8e avenue\n",
    "   Addresse_list[i] = re.sub(r'^(\\d+), (\\d+) Avenue', r'\\1, \\2e Avenue', Addresse_list[i])\n",
    "   Addresse_list[i] = re.sub(r'^(\\d+), (\\d+) Rue', r'\\1, \\2e Rue', Addresse_list[i])\n",
    "\n",
    "   #TODO Nettoyer pour les appartements, les retirer exemple (123, app. 808, avneue xyz --> 123, avenue xyz)\n",
    "   Addresse_list[i] = re.sub(r'app\\..*?, ', '', Addresse_list[i])\n",
    "\n",
    "   #TODO Retirer les Abbrévations de Pointes-aux-trembles et RDP des addresses\n",
    "   Addresse_list[i] =Addresse_list[i].replace(\" (R.-d.-P.)\",\"\")\n",
    "   Addresse_list[i]=Addresse_list[i].replace(\" (P.-a.-T.)\",\"\")\n",
    "\n",
    "   #TODO Retirer les \"quartier...\" Exemple( 123, avenue xyz, Dorval, Quartier Sud --> 123, avenue xyz, Dorval)\n",
    "   Addresse_list[i] = re.sub(\", Quartier .*, \", \", \",Addresse_list[i])\n",
    "\n",
    "   #TODO Retirer les addresses double (123-125 avenue xyz --> 123, avenue xyz)\n",
    "   Addresse_list[i] = re.sub(r' - \\d+', '', Addresse_list[i])\n",
    "\n",
    "   #TODO Nettoyer pour les appartements, les retirer exemple (123, app. 808, avneue xyz --> 123, avenue xyz)\n",
    "   Addresse_list[i] = (re.sub(r'app\\.\\s*([A-Za-z\\d]+, )', '', Addresse_list[i]))\n",
    "   match = re.search(r\"app\\.\", Addresse_list[i])\n",
    "   if match:\n",
    "      Addresse_list[i] = re.sub(r\"app\\.\\s\\d+, \", \"\", Addresse_list[i])\n",
    "\n",
    "   #TODO enlever les d'\n",
    "   Addresse_list[i] = Addresse_list[i].replace(\" d'\",\" \")\n",
    "Propriétés['Addresse'] = Addresse_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the function to extract different interest place around\n",
    "def get_place_data(url,df):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(f\"{url}{1}\")\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    container = soup.find_all(\"div\", {\"class\":\"col-auto\"})\n",
    "    for i in container:\n",
    "        if i.find(\"h2\") is not None:\n",
    "            result = i.find(\"h2\")\n",
    "    #result= [i.find(\"h2\") for i in container if i.find(\"h2\") is not None]\n",
    "    stringresult = (result.text).split(\"sur \")\n",
    "    intresult = int((stringresult[1]).split(\" \")[0])\n",
    "    iterations = ((intresult//100)+2)\n",
    "    quartier = []\n",
    "    centers = []\n",
    "    location = []\n",
    "    for i in range(1,iterations):\n",
    "        time.sleep(5)\n",
    "        driver.get(f\"{url}{i}\")\n",
    "        # Récupérez le contenu HTML de la page\n",
    "        html = driver.page_source\n",
    "        # Utilisez BeautifulSoup pour parser le contenu HTML\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        all_centers = (soup.find_all(\"div\", {\"class\": \"list-group-item-title\"}))\n",
    "        centers.extend([center.text for center in all_centers])\n",
    "        address_content = (soup.find_all(\"div\", {\"class\": \"list-group-item-content\"}))\n",
    "        for address in address_content:\n",
    "            if len(address) == 1:\n",
    "                city = address.find_next(\"div\")\n",
    "            elif len(address) == 2:\n",
    "                loc = address.find_next(\"div\")\n",
    "                city = loc.find_next(\"div\")\n",
    "            elif len(address) == 3:\n",
    "                badge = address.find_next(\"div\")\n",
    "                loc = badge.find_next(\"div\")\n",
    "                city = loc.find_next(\"div\")\n",
    "            quartier.append(city.text)\n",
    "            location.append(loc.text)\n",
    "    df = pd.DataFrame({\"Centre\": centers, \"Addresse\": location, \"Quartier\": quartier})\n",
    "    return df\n",
    "\n",
    "##TODO Transforme l'adresse en Point géographique (Lat,Lon,Alt) 123 avenue xyz, montréal --> (-73,45,0)\n",
    "def get_location(df):\n",
    "    locator = Nominatim(user_agent=\"myGeocoder\", timeout=10)\n",
    "    df['location'] = df['Addresse'].apply(locator.geocode)\n",
    "    df['point'] = df['location'].apply(lambda loc: tuple(loc.point) if loc else None)\n",
    "\n",
    "\n",
    "    return df\n",
    "#TODO Découper le point géogrpahique en 3 colonnes séparés\n",
    "def get_lat_lon(df):\n",
    "    df[['Lat', 'Lon', 'Alt']] = df['point'].apply(pd.Series)\n",
    "    df['Lat'] = df['Lat'].astype(float)\n",
    "    df['Lon'] = df['Lon'].astype(float)\n",
    "    df = df.drop(columns=['point', 'Alt'])\n",
    "    return df\n",
    "\n",
    "#TODO Aller chercher les limites geJSOM d'un quartier et trouver dans quel qaurtier appartient \"x\" point géogrpahique Lon,Lat Exemple -73.5,45.22 --> Verdun\n",
    "def Link_map_data(df):\n",
    "    df[\"geometry\"] = gpd.points_from_xy(df[\"Lon\"], df[\"Lat\"])\n",
    "    df = gpd.GeoDataFrame(df, geometry=\"geometry\")\n",
    "    df1 = gpd.sjoin(df, neighborhoods, how=\"left\", op='within')\n",
    "# Ajoutez une colonne de quartier à votre dataframe de df\n",
    "    df[\"NOM\"] = df1[\"NOM\"]\n",
    "    if 'location' in df.columns:\n",
    "        df =df.drop(columns=['location'])\n",
    "    if 'geometry' in df.columns:\n",
    "        df = df.drop(columns=['geometry'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def apply_data_map(dforiginal,dfgrouped,dffinal):\n",
    "    dffinal = pd.merge(dforiginal,dfgrouped, on =\"NOM\", how=\"inner\")\n",
    "    return\n",
    "\n",
    "Prop_map = get_location(Propriétés)\n",
    "Prop_map = get_lat_lon(Prop_map)\n",
    "Prop_map = Link_map_data(Prop_map)\n",
    "Prop_map.to_csv(\"Export/Houses2V1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call the function on interest places\n",
    "Sportscenters = pd.DataFrame\n",
    "sportsurl = \"https://montreal.ca/lieux?filter=true&mtl_content.lieux.category.code=CENT&page=\"\n",
    "Sportscenters = get_place_data(sportsurl,Sportscenters)\n",
    "Sportscenters[\"Addresse\"] = Sportscenters[\"Addresse\"].apply(lambda x: x + \", Montréal\")\n",
    "Sportscenters = get_location(Sportscenters)\n",
    "Sportscenters=get_lat_lon(Sportscenters)\n",
    "Sportscenters = Link_map_data(Sportscenters)\n",
    "\n",
    "\n",
    "\n",
    "Communitycenters = pd.DataFrame\n",
    "communityurl = \"https://montreal.ca/lieux?mtl_content.lieux.category.code=BIBL&page=\"\n",
    "Communitycenters = get_place_data(communityurl,Communitycenters)\n",
    "Communitycenters[\"Addresse\"] = Communitycenters[\"Addresse\"].apply(lambda x: x + \", Montréal\")\n",
    "Communitycenters = get_location(Communitycenters)\n",
    "Communitycenters=get_lat_lon(Communitycenters)\n",
    "Communitycenters = Link_map_data(Communitycenters)\n",
    "\n",
    "\n",
    "Parcs = pd.DataFrame\n",
    "parcsurl = \"https://montreal.ca/lieux?filter=true&mtl_content.lieux.category.code=PARC&page=\"\n",
    "Parcs = get_place_data(parcsurl,Parcs)\n",
    "Parcs[\"Addresse\"] = Parcs[\"Addresse\"].apply(lambda x: x + \", Montréal\")\n",
    "Parcs = get_location(Parcs)\n",
    "Parcs=get_lat_lon(Parcs)\n",
    "Parcs = Link_map_data(Parcs)\n",
    "\n",
    "\n",
    "\n",
    "Mairies = pd.DataFrame()\n",
    "mairiesurl = \"https://montreal.ca/lieux?mtl_content.lieux.installation.code=MAIR&PAGE=\"\n",
    "Mairies = get_place_data(mairiesurl,Mairies)\n",
    "Mairies = Mairies[Mairies['Centre'].str.contains(\"Mairie d'arrondissement\")].reset_index().drop(columns=[\"index\"])\n",
    "Mairies['Addresse'].replace([\"5160, boul. Décarie, bureau 600\",\"201 de l'avenue Laurier Est\"], [\"5160, boul. Décarie, Montreal\", \"201, avenue Laurier Est, Montreal\"], inplace=True)\n",
    "Mairies = get_location(Mairies)\n",
    "Mairies = get_lat_lon(Mairies)\n",
    "Mairies = Link_map_data(Mairies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parcs.to_csv(\"Export/Parcs.csv\")\n",
    "Communitycenters.to_csv(\"Export/Communitycenters.csv\")\n",
    "Sportscenters.to_csv(\"Export/Sportscenters.csv\")\n",
    "Mairies.to_csv(\"Export/Mairies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_restaurants = pd.concat([Coffee,Greek,Italy,Africain,Antillais,Asian,Mexicain,Healthy])\n",
    "#TODO Drop duplicates si toutes les colonnes sauf la catégogire de l'origine du restaurant est pareille.\n",
    "new_restaurants = new_restaurants.drop_duplicates(subset=[\"Nom\",\"Rue\",\"postalCode\",\"Latitude\",\"Longitude\",\"reviewsCount\",\"totalScore\"])\n",
    "new_restaurants = new_restaurants.rename(columns={'Longitude':'Lon','Latitude':'Lat'})\n",
    "Link_map_data(new_restaurants)\n",
    "nbNew = new_restaurants.groupby(\"NOM\").count()[[\"Nom\"]]\n",
    "nbNew = nbNew.rename(columns={\"Nom\":\"Count of Restaurant\"})\n",
    "new_restaurants = new_restaurants.dropna(subset=[\"NOM\"])\n",
    "new_restaurants = new_restaurants.drop(columns=[\"geometry\"])\n",
    "new_restaurants.to_csv(\"Export/Restaurants.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Drop data on houses where either name, number of rooms or price is NaN\n",
    "price_per_piece = Prop_map.dropna(subset=[\"piece\",\"NOM\",\"price\"], inplace=False)\n",
    "#TODO 123 456,54$ type String --> 123456 type INT\n",
    "clean_price = [int((re.sub(r'\\u00A0', ' ', r[2])).replace(\" \",\"\").replace(\"$\",\"\")) for i,r in price_per_piece.iterrows()]\n",
    "price_per_piece[\"price\"] = clean_price\n",
    "#TODO retirer pièce du nombre de pièces --> 13 pièces --> 13\n",
    "clean_piece = [int(r[3].replace(\" pièce\",\"\").replace(\"s\",\"\")) for i,r in price_per_piece.iterrows()]\n",
    "price_per_piece['piece'] = clean_piece\n",
    "#TODO Prix/pièce\n",
    "price_per_piece[\"Price_per_piece\"] = (price_per_piece[\"price\"]/price_per_piece[\"piece\"]).astype(int)\n",
    "\n",
    "price_per_piece = price_per_piece.groupby(\"NOM\").mean(\"Price_per_piece\")[[\"Price_per_piece\"]].astype(int).sort_values(by=\"Price_per_piece\",ascending = False).reset_index()\n",
    "price_per_piece.to_csv(\"Export/Price_per_piece.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_per_km = pd.merge(superficie[[\"NOM\",\"Superficie (km2)\"]],nbNew,on=\"NOM\")\n",
    "rest_per_km[\"Restaurant_by_km2\"] = (rest_per_km[\"Count of Restaurant\"]/rest_per_km[\"Superficie (km2)\"]).apply(math.ceil)\n",
    "rest_per_km.to_csv(\"Export/Rest_per_KM.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "975009d07ecb89591f421a82b28366f5e95ebda4302b5da20f317cfbfc51cb08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
